# -*- coding: utf-8 -*-
"""Entrenamiento (Multiclase) new

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/159hj1QVurQfQCqsC3Zh72avGMUL_thgR
"""

!pip install timm
!pip install wandb
!pip install torchinfo
!pip install --upgrade matplotlib

## Permisos para acceder a la ubicación
from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import nibabel as nib
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
import timm
import seaborn as sns
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import random
import  sklearn.metrics
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix
from sklearn.metrics import matthews_corrcoef
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Subset
import wandb
from torch.optim.lr_scheduler import CosineAnnealingLR


# Definir el dispositivo
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Ruta del conjunto de datos
output_dir='/content/drive/MyDrive/SartajDataset'

# Crear directorio de salida si no existe
os.makedirs(output_dir, exist_ok=True)

def visualize_images(class_name, num_images=4):
    class_path = os.path.join(output_dir, class_name)
    images = [img for img in os.listdir(class_path) if img.endswith('.jpg')]

    plt.figure(figsize=(10, 5))
    for i in range(min(num_images, len(images))):
        img_path = os.path.join(class_path, images[i])
        img = Image.open(img_path)
        plt.subplot(1, num_images, i + 1)
        plt.imshow(img, cmap='gray')
        plt.axis('off')
        plt.title(images[i])

    plt.show()

# Visualizar algunas imágenes de HGG y LGG
visualize_images('/content/drive/MyDrive/SartajDataset/Training/glioma_tumor', num_images=4)
visualize_images('/content/drive/MyDrive/SartajDataset/Training/meningioma_tumor', num_images=4)
visualize_images('/content/drive/MyDrive/SartajDataset/Training/no_tumor', num_images=4)
visualize_images('/content/drive/MyDrive/SartajDataset/Training/pituitary_tumor', num_images=4)

import os
from torch.utils.data import Dataset
from PIL import Image

class CustomDataset(Dataset):
    def __init__(self, glioma_dir, meningioma_dir, no_tumor_dir, pituitary_dir, transform=None):
        """
        Inicializa el dataset.

        :param glioma_dir: Directorio de imágenes de la clase glioma_tumor.
        :param meningioma_dir: Directorio de imágenes de la clase meningioma_tumor.
        :param no_tumor_dir: Directorio de imágenes de la clase no_tumor.
        :param pituitary_dir: Directorio de imágenes de la clase pituitary_tumor.
        :param transform: Transformaciones a aplicar a las imágenes.
        """
        self.glioma_dir = glioma_dir
        self.meningioma_dir = meningioma_dir
        self.no_tumor_dir = no_tumor_dir
        self.pituitary_dir = pituitary_dir
        self.transform = transform

        # Listas para almacenar las rutas de las imágenes y las etiquetas correspondientes
        self.images = []
        self.labels = []

        # Cargar imágenes y etiquetas
        self._load_images()

    def _load_images(self):
        """
        Carga las imágenes de los directorios y asigna etiquetas.
        """
        # Cargar imágenes de glioma_tumor (etiqueta 0)
        for img_name in os.listdir(self.glioma_dir):
            img_path = os.path.join(self.glioma_dir, img_name)
            self.images.append(img_path)
            self.labels.append(0)  # Etiqueta 0 para glioma_tumor

        # Cargar imágenes de meningioma_tumor (etiqueta 1)
        for img_name in os.listdir(self.meningioma_dir):
            img_path = os.path.join(self.meningioma_dir, img_name)
            self.images.append(img_path)
            self.labels.append(1)  # Etiqueta 1 para meningioma_tumor

        # Cargar imágenes de no_tumor (etiqueta 2)
        for img_name in os.listdir(self.no_tumor_dir):
            img_path = os.path.join(self.no_tumor_dir, img_name)
            self.images.append(img_path)
            self.labels.append(2)  # Etiqueta 2 para no_tumor

        # Cargar imágenes de pituitary_tumor (etiqueta 3)
        for img_name in os.listdir(self.pituitary_dir):
            img_path = os.path.join(self.pituitary_dir, img_name)
            self.images.append(img_path)
            self.labels.append(3)  # Etiqueta 3 para pituitary_tumor

    def __len__(self):
        """
        Devuelve el número total de imágenes en el dataset.
        """
        return len(self.images)

    def __getitem__(self, idx):
        """
        Devuelve una imagen y su etiqueta correspondiente.

        :param idx: Índice de la imagen que se desea obtener.
        :return: Tupla (imagen, etiqueta).
        """
        # Obtener la ruta de la imagen y cargarla
        img_path = self.images[idx]
        img = Image.open(img_path).convert("RGB")  # Asegurarse de que la imagen sea RGB

        # Obtener la etiqueta correspondiente
        label = self.labels[idx]

        # Aplicar la transformación si está definida
        if self.transform:
            img = self.transform(img)

        return img, label

import os
from torch.utils.data import Dataset
from PIL import Image

class CustomTestDataset(Dataset):
    def __init__(self, glioma_test_dir, meningioma_test_dir, no_tumor_test_dir, pituitary_test_dir, transform=None):
        """
        Inicializa el dataset de prueba con el 100% de las imágenes de cada clase.

        :param glioma_test_dir: Directorio de imágenes de la clase glioma_tumor para prueba.
        :param meningioma_test_dir: Directorio de imágenes de la clase meningioma_tumor para prueba.
        :param no_tumor_test_dir: Directorio de imágenes de la clase no_tumor para prueba.
        :param pituitary_test_dir: Directorio de imágenes de la clase pituitary_tumor para prueba.
        :param transform: Transformaciones a aplicar a las imágenes.
        """
        self.glioma_test_dir = glioma_test_dir
        self.meningioma_test_dir = meningioma_test_dir
        self.no_tumor_test_dir = no_tumor_test_dir
        self.pituitary_test_dir = pituitary_test_dir
        self.transform = transform

        # Listas para almacenar las rutas de las imágenes y las etiquetas correspondientes
        self.images = []
        self.labels = []

        # Cargar el 100% de las imágenes de cada clase
        self._load_images()

    def _load_images(self):
        """
        Carga el 100% de las imágenes de cada directorio y asigna etiquetas.
        """
        def load_images_from_dir(directory, label):
            all_images = os.listdir(directory)
            for img_name in all_images:
                img_path = os.path.join(directory, img_name)
                self.images.append(img_path)
                self.labels.append(label)

        # Cargar imágenes de cada clase con su etiqueta correspondiente
        load_images_from_dir(self.glioma_test_dir, 0)      # Glioma tumor
        load_images_from_dir(self.meningioma_test_dir, 1)  # Meningioma tumor
        load_images_from_dir(self.no_tumor_test_dir, 2)    # No tumor
        load_images_from_dir(self.pituitary_test_dir, 3)   # Pituitary tumor

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img = Image.open(self.images[idx]).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return img, self.labels[idx]

##Verificar numero de imagenes en training folder
glioma_tumor_dir = '/content/drive/MyDrive/SartajDataset/Training/glioma_tumor'
meningioma_tumor_dir = '/content/drive/MyDrive/SartajDataset/Training/meningioma_tumor'
no_tumor_dir= '/content/drive/MyDrive/SartajDataset/Training/no_tumor'
pituitary_tumor_dir= '/content/drive/MyDrive/SartajDataset/Training/pituitary_tumor'

# Contar imágenes en glioma
glioma_images = os.listdir(glioma_tumor_dir)
glioma_count = len(glioma_images)

# Contar imágenes en glioma
meningioma_images = os.listdir(meningioma_tumor_dir)
meningioma_count = len(meningioma_images)

# Contar imágenes en glioma
no_tumor_images = os.listdir(no_tumor_dir)
no_tumor_count = len(no_tumor_images)

# Contar imágenes en glioma
pituitary_images = os.listdir(pituitary_tumor_dir)
pituitary_count = len(pituitary_images)


# Imprimir resultados
print(f'Número de imágenes en Glioma file: {glioma_count}')
print(f'Número de imágenes en Meningioma file: {meningioma_count}')
print(f'Número de imágenes en No_tumor file: {no_tumor_count}')
print(f'Número de imágenes en Pituitary file: {pituitary_count}')

# Definir transformaciones para el conjunto de training
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Ajusta el tamaño
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Cambiar brillo, contraste, etc.
    transforms.RandomRotation(45),  # Rotación aleatoria
    transforms.RandomHorizontalFlip(),    #Se voltea la imagen horizontalmente
    transforms.RandomVerticalFlip(),   #Se voltea verticalmente
   # transforms.RandomPerspective(),
    transforms.ToTensor()
    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalización de ImageNet
])

#Definir transformaciones para el conjunto de testing
# Transformaciones para el conjunto de prueba
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalización de ImageNet
])

# Cargar el conjunto de datos completo
train_dataset = CustomDataset(glioma_tumor_dir, meningioma_tumor_dir, no_tumor_dir, pituitary_tumor_dir,  transform=train_transform)

import os

# Verificar número de imágenes en testing folder
glioma_tumor_test_dir = '/content/drive/MyDrive/SartajDataset/Testing/glioma_tumor'
meningioma_tumor_test_dir = '/content/drive/MyDrive/SartajDataset/Testing/meningioma_tumor'
no_tumor_test_dir = '/content/drive/MyDrive/SartajDataset/Testing/no_tumor'
pituitary_tumor_test_dir = '/content/drive/MyDrive/SartajDataset/Testing/pituitary_tumor'

# Contar imágenes en glioma
glioma_images = os.listdir(glioma_tumor_test_dir)
glioma_count = len(glioma_images)

# Contar imágenes en meningioma
meningioma_images = os.listdir(meningioma_tumor_test_dir)
meningioma_count = len(meningioma_images)

# Contar imágenes en no_tumor
no_tumor_images = os.listdir(no_tumor_test_dir)
no_tumor_count = len(no_tumor_images)

# Contar imágenes en pituitary
pituitary_images = os.listdir(pituitary_tumor_test_dir)
pituitary_count = len(pituitary_images)

# Imprimir resultados
print(f'Número de imágenes en Glioma file (Testing): {glioma_count}')
print(f'Número de imágenes en Meningioma file (Testing): {meningioma_count}')
print(f'Número de imágenes en No_tumor file (Testing): {no_tumor_count}')
print(f'Número de imágenes en Pituitary file (Testing): {pituitary_count}')

##Cargar el conjunto de prueba
test_dataset = CustomTestDataset(glioma_tumor_test_dir, meningioma_tumor_test_dir, no_tumor_test_dir, pituitary_tumor_test_dir, transform=test_transform)

# Número de imágenes a mostrar
num_images_to_show = 5

# Crear la figura con subgráficos en una fila
fig, axes = plt.subplots(1, num_images_to_show, figsize=(10, 10))

# Mostrar algunas imágenes transformadas
for i in range(num_images_to_show):
    # Seleccionar una imagen aleatoria y su etiqueta
    idx = torch.randint(0, len(train_dataset), (1,)).item()  # Generar un índice aleatorio
    image, label = train_dataset[idx]  # Obtener la imagen y la etiqueta

    # Convertir la imagen de tensor (C, H, W) a (H, W, C) para la visualización
    image = image.permute(1, 2, 0).numpy()  # Convertir a formato (H, W, C)

    # Mostrar la imagen en el subplot correspondiente
    ax = axes[i]  # Obtener el eje para esta imagen
    ax.imshow(image)
    ax.set_title(f'Label: {label}')
    ax.axis('off')  # Desactivar los ejes

# Ajustar el layout para que no se solapen las imágenes
plt.tight_layout()
plt.show()

import torch
from torch.utils.data import random_split

# Porcentaje configurable (por ejemplo: 10%)
subset_percentage = 0.1  # 10%

# Número mínimo de muestras para el conjunto de prueba (opcional)
min_test_samples = 20

# Calcular número de muestras para el subconjunto de entrenamiento
num_train_samples = int(len(train_dataset) * subset_percentage)

# Calcular número de muestras para el subconjunto de prueba (con mínimo)
num_test_samples = max(int(len(test_dataset) * subset_percentage), min_test_samples)

# Dividir aleatoriamente el dataset de entrenamiento
train_subset, _ = random_split(train_dataset, [num_train_samples, len(train_dataset) - num_train_samples])

# Dividir aleatoriamente el dataset de prueba
test_subset, _ = random_split(test_dataset, [num_test_samples, len(test_dataset) - num_test_samples])

# Verificar los tamaños de los subconjuntos
print(f"Tamaño del subconjunto de entrenamiento (train_subset): {len(train_subset)}")
print(f"Tamaño del subconjunto de prueba (test_subset): {len(test_subset)}")

# Crear los DataLoaders
train_loader = DataLoader(train_subset, batch_size= 32, shuffle=True)
test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)

# Verificar el tamaño de los DataLoaders
print(f'Tamaño de los batches en el DataLoader de entrenamiento: {len(train_loader)}')
print(f'Tamaño de los batches en el DataLoader de prueba: {len(test_loader)}')

for images, labels in train_loader:
    print("Imagenes Shape:", images.shape)  # Debería ser [batch_size, channels, height, width]
    print("Etiquetas Shape:", labels.shape)  # Debería ser [batch_size] para clasificación

    images, labels = images.to(device), labels.to(device)  # Mover a GPU si es necesario
    break  # Salir

# Comprobar si hay una GPU disponible y usarla
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Definir el modelo

model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=4)

model = model.to(device)   ### inicializa con pesos preentrenados en ImageNet+

# Definir el optimizador
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Ajusta la tasa de aprendizaje aquí  lr=1e-4

# Definir la función de pérdida
criterion = nn.CrossEntropyLoss()
  # Para clasificación binaria

# Imprimir la configuración
print(f"Función de pérdida: {criterion}")
print(f"Optimizador: {optimizer}")

# Pasamos las imágenes al modelo
outputs = model(images)

# Verificamos las dimensiones de las salidas
print("Salidas Shape:", outputs.shape)  # Debería ser [batch_size, num_classes]

# Verificamos que las salidas no sean NaN ni Inf
if torch.isnan(outputs).any() or torch.isinf(outputs).any():
    print("¡Hay NaNs o Inf en las salidas!")
else:
    print("Las salidas son válidas.")

# Si estamos usando CrossEntropyLoss, los outputs deberían ser logits (sin pasar por softmax aún)

import numpy as np
import cv2
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

# Función para obtener las activaciones de las capas usando hooks
def register_hooks(model):
    activations = {}

    def save_activation(name):
        def hook(module, input, output):
            activations[name] = output.detach()
        return hook

    # Buscar la última capa convolucional del modelo
    last_conv_layer = None
    for name, layer in model.named_modules():
        if isinstance(layer, torch.nn.Conv2d):
            last_conv_layer = layer  # Guarda la última capa convolucional

    if last_conv_layer is None:
        raise ValueError("No se encontró una capa convolucional en el modelo.")

    # Registrar el hook en la salida de la capa convolucional
    last_conv_layer.register_forward_hook(save_activation('last_conv'))

    return activations

# Función para generar Grad-CAM
def generate_grad_cam(model, input_image, class_index, activations, device):
    # Asegurarse de que la imagen esté en el dispositivo correcto
    input_image = input_image[0].unsqueeze(0).to(device)  # Tomamos solo una imagen del batch

    # Desactivar gradientes para la imagen de entrada
    input_image.requires_grad = True

    # Realizar la predicción
    output = model(input_image)

    # Crear un tensor de gradientes del tamaño adecuado
    grad_output = torch.zeros_like(output)
    grad_output[0][class_index] = 1  # Establecemos la clase deseada para el Grad-CAM

    # Retropropagar los gradientes
    model.zero_grad()
    output.backward(grad_output, retain_graph=True)

    # Obtener los gradientes sobre la imagen
    gradients = input_image.grad  # Gradientes con respecto a la imagen de entrada

    # Obtener las activaciones de la última capa convolucional
    feature_map = activations['last_conv']

    # Redimensionar los gradientes
    gradients_resized = F.interpolate(gradients, size=(feature_map.shape[2], feature_map.shape[3]), mode='bilinear', align_corners=False)

    # Promediar los gradientes sobre los canales
    pooled_gradients = torch.mean(gradients_resized, dim=1, keepdim=True)

    # Expandir los gradientes para que coincidan con las activaciones
    pooled_gradients = pooled_gradients.expand_as(feature_map)

    # Multiplicar las activaciones por los gradientes ponderados
    weighted_activations = pooled_gradients * feature_map

    # Sumar a través de los canales para obtener el mapa de activación
    grad_cam_map = torch.sum(weighted_activations, dim=1).squeeze()

    # Asegurarnos de que el grad_cam_map sea 2D
    grad_cam_map = grad_cam_map.cpu().detach().numpy()
    grad_cam_map = np.maximum(grad_cam_map, 0)  # Eliminar valores negativos
    grad_cam_map = cv2.resize(grad_cam_map, (input_image.size(3), input_image.size(2)))  # Redimensionamos al tamaño de la imagen
    grad_cam_map -= np.min(grad_cam_map)
    grad_cam_map /= np.max(grad_cam_map)

    return grad_cam_map


# Función para mostrar Grad-CAM
def show_grad_cam(grad_cam_map, input_image, class_name, colormap=cv2.COLORMAP_JET):
    # Asegúrese de que la imagen de entrada esté en el formato correcto
    img = input_image[0].cpu().detach().numpy()
    img = np.transpose(img, (1, 2, 0))  # De [C, H, W] a [H, W, C]
    img = np.uint8(255 * img)  # Convertimos la imagen a formato uint8

    # Redimensionar Grad-CAM
    grad_cam_map_resized = cv2.resize(grad_cam_map, (img.shape[1], img.shape[0]))  # Redimensionar Grad-CAM
    grad_cam_map_resized = np.maximum(grad_cam_map_resized, 0)
    grad_cam_map_resized -= np.min(grad_cam_map_resized)
    grad_cam_map_resized /= np.max(grad_cam_map_resized)

    # Aplicar el colormap
    heatmap = cv2.applyColorMap(np.uint8(255 * grad_cam_map_resized), colormap)

    # Superponer Grad-CAM sobre la imagen original
    superimposed_img = cv2.addWeighted(heatmap, 0.4, img, 0.6, 0)

    # Mostrar la imagen con el nombre de la clase
    plt.imshow(superimposed_img)
    plt.title(f'Grad-CAM: {class_name}')
    plt.axis('off')
    plt.show()
     # Registrar en WandB
    wandb.log({"Test Grad-CAM": wandb.Image(superimposed_img)})


# Función para generar Grad-CAM para cada clase
def generate_and_show_grad_cam_for_all_classes(model, input_image, activations, device, class_names):
    # Obtener el número de clases
    num_classes = len(class_names)  # Usualmente, el número de clases está en el tamaño de `class_names`

    # Iterar sobre todas las clases
    for class_index in range(num_classes):
        grad_cam_map = generate_grad_cam(model, input_image, class_index, activations, device)
        show_grad_cam(grad_cam_map, input_image, class_names[class_index])

import torch
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef
import wandb
import seaborn as sns
import matplotlib.pyplot as plt

# Obtener etiquetas del subset (asumiendo que train_subset[i] devuelve (imagen, etiqueta))
train_targets = [train_subset[i][1] for i in range(len(train_subset))]

# Definir el número de pliegues
num_folds = 5
skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

# Ciclo de Stratified K-Fold
for fold, (train_idx, val_idx) in enumerate(skf.split(train_subset, train_targets)):
    wandb.init(project="SARTAJ Dataset training", name=f"Fold_{fold + 1}")

    # Configurar los datos de entrenamiento y validación para este pliegue
    train_fold_subset = torch.utils.data.Subset(train_subset, train_idx)
    val_fold_subset = torch.utils.data.Subset(train_subset, val_idx)

    # Crear los DataLoaders
    train_loader = DataLoader(train_fold_subset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_fold_subset, batch_size=32, shuffle=False)

    # Reiniciar el modelo y optimizador para cada pliegue
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

    num_epochs = 10
    train_confusion_matrices = []
    test_confusion_matrices = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        all_labels = []
        all_outputs = []
        all_probs = []

        for images, labels in train_loader:
            labels = labels.long()
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

            all_labels.extend(labels.cpu().numpy())
            pred = torch.argmax(outputs, dim=1).detach().cpu().numpy()
            all_outputs.extend(pred)
            probs = torch.softmax(outputs, dim=1).detach().cpu().numpy()
            all_probs.extend(probs)

        train_accuracy = accuracy_score(all_labels, all_outputs)
        train_precision = precision_score(all_labels, all_outputs, average='macro')
        train_recall = recall_score(all_labels, all_outputs, average='macro')
        train_f1 = f1_score(all_labels, all_outputs, average='macro')
        train_cm = confusion_matrix(all_labels, all_outputs)
        train_confusion_matrices.append(train_cm)
        train_mcc = matthews_corrcoef(all_labels, all_outputs)

        wandb.log({
            "Fold": fold + 1,
            "Epoch": epoch + 1,
            "Loss": running_loss / len(train_loader),
            "Accuracy": train_accuracy,
            "Precision": train_precision,
            "Recall": train_recall,
            "F1 Score": train_f1,
            "MCC": train_mcc,
            "Learning Rate": optimizer.param_groups[0]['lr'],
        })
         # Mostrar resultados de entrenamiento
        print(f"Fold [{fold + 1}], Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, "
              f"Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, "
              f"Recall: {train_recall:.4f}, F1 Score: {train_f1:.4f}, MCC: {train_mcc:.4f}")
        print(f"Learning Rate: {optimizer.param_groups[0]['lr']}")


        # === Validación ===
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            all_val_labels = []
            all_val_outputs = []
            all_val_probs = []

            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                all_val_labels.extend(labels.cpu().numpy())
                val_pred = torch.argmax(outputs, dim=1).detach().cpu().numpy()
                all_val_outputs.extend(val_pred)
                val_probs = torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()
                all_val_probs.extend(val_probs)

            val_accuracy = accuracy_score(all_val_labels, all_val_outputs)
            val_precision = precision_score(all_val_labels, all_val_outputs, average='macro')
            val_recall = recall_score(all_val_labels, all_val_outputs, average='macro')
            val_f1 = f1_score(all_val_labels, all_val_outputs, average='macro')
            val_mcc = matthews_corrcoef(all_val_labels, all_val_outputs)

            wandb.log({
                "Fold": fold + 1,
                "Validation Loss": val_loss / len(val_loader),
                "Validation Accuracy": val_accuracy,
                "Validation Precision": val_precision,
                "Validation Recall": val_recall,
                "Validation F1 Score": val_f1,
                "Validation MCC": val_mcc,
            })
            # Mostrar resultados de validación
            print(f"Fold [{fold + 1}], Validation Loss: {val_loss / len(val_loader):.4f}, "
                  f"Validation Accuracy: {val_accuracy:.4f}, Validation Precision: {val_precision:.4f}, "
                  f"Validation Recall: {val_recall:.4f}, Validation F1 Score: {val_f1:.4f}, "
                  f"Validation MCC: {val_mcc:.4f}")  # Mostrar MCC de la validación)

        scheduler.step(val_loss)

        # === Test ===
        model.eval()
        with torch.no_grad():
            all_test_labels = []
            all_test_outputs = []
            all_test_probs = []
            test_loss = 0.0

            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                test_loss += loss.item()

                all_test_labels.extend(labels.cpu().numpy())
                test_pred = torch.argmax(outputs, dim=1).detach().cpu().numpy()
                all_test_outputs.extend(test_pred)
                test_probs = torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()
                all_test_probs.extend(test_probs)

            test_accuracy = accuracy_score(all_test_labels, all_test_outputs)
            test_precision = precision_score(all_test_labels, all_test_outputs, average='macro')
            test_recall = recall_score(all_test_labels, all_test_outputs, average='macro')
            test_f1 = f1_score(all_test_labels, all_test_outputs, average='macro')
            test_mcc = matthews_corrcoef(all_test_labels, all_test_outputs)
            test_cm = confusion_matrix(all_test_labels, all_test_outputs)
            test_confusion_matrices.append(test_cm)

            wandb.log({
                "Fold": fold + 1,
                "Test Loss": test_loss / len(test_loader),
                "Test Accuracy": test_accuracy,
                "Test Precision": test_precision,
                "Test Recall": test_recall,
                "Test F1 Score": test_f1,
                "Test MCC": test_mcc,
            })
             # Mostrar resultados de prueba
            print(f"Fold [{fold + 1}], Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.4f}, "
                  f"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, "
                  f"Test F1 Score: {test_f1:.4f}, Test MCC: {test_mcc:.4f}")  # Mostrar MCC")

    # === Grad-CAM por clase ===
    print(f"Generando Grad-CAM por clase para el Fold {fold + 1}...")
    class_names = ['Clase 0', 'Clase 1', 'Clase 2', 'Clase 3']
    images_per_class = {}

    # Buscar una imagen por clase en el test set
    for images, labels in test_loader:
        for img, lbl in zip(images, labels):
            cls = lbl.item()
            if cls not in images_per_class:
                images_per_class[cls] = img.unsqueeze(0).to(device)
            if len(images_per_class) == len(class_names):
                break
        if len(images_per_class) == len(class_names):
            break

    # Generar Grad-CAM para cada clase
    for cls_idx in range(len(class_names)):
      if cls_idx in images_per_class:
        input_image = images_per_class[cls_idx]
        activations = register_hooks(model)
        grad_cam_map = generate_grad_cam(model, input_image, cls_idx, activations, device)
        show_grad_cam(grad_cam_map, input_image.cpu(), class_names[cls_idx])


    # Matriz de confusión final del fold
    plt.figure(figsize=(6, 6))
    sns.heatmap(test_confusion_matrices[-1], annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f'Test Confusion Matrix for Fold {fold + 1}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    wandb.log({"Test Confusion Matrix": wandb.Image(plt)})
    plt.show()

    wandb.finish()